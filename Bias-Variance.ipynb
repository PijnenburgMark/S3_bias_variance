{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433c4533-41d0-4333-a022-a41d4ac092b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Bias - Variance Tradeoff "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28296a-31bf-4e37-8ab5-5981ab7d60e2",
   "metadata": {},
   "source": [
    "We beginnen met de initialisatie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96a11b2-9a06-4b5a-a7e5-0228138705e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading numpy-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
      "Successfully installed joblib-1.4.2 numpy-2.2.2 scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scikit-learn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a194be-715d-4b88-aaf0-6691a2896bee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In dit workbook gaan we het hebben over de 3 verschillende oorzaken waarom een voorspellend model niet perfect voorspelt op een test set. De 3 redenen zijn:\n",
    "1. de irreducibele fout,\n",
    "2. de bias,\n",
    "3. de variance.\n",
    "\n",
    "De irreducibele fout is lastig te voorkomen. Als we een targetvariabele $y$ voorspellen op basis van $x$, dan kan het zijn dat er nog veel andere factoren zijn die $y$ ook beïnvloeden, maar waar we geen data van hebben. \n",
    "Als we bijvoorbeeld het inkomen voorspellen op basis van het opleidingsniveau, krijgen we nooit een perfecte voorspelling, omdat bijvoorbeeld leeftijd ook een rol speelt. \n",
    "Dit wordt meestal al duidelijk in de data set: er komen twee personen voor met hetzelfde opleidingsniveau en toch hebben beiden een ander inkomen. Dit heet de irreducibele fout. De enige manier om de irreducibele fout \n",
    "kleiner te maken is om meer voorspellende variabelen mee te nemen, maar dan moet je daar dus ook data van hebben. \n",
    "\n",
    "De bias is een afwijking in de voorspelling, omdat het gekozen model te eenvoudig is om het werkelijke verband tussen $x$ en $y$ precies te beschrijven. Als je bijvoorbeeld met lineaire regressie de tijd wilt voorspellen \n",
    "dat een voorwerp nodig heeft om van een hooge $h$ op de vloer te vallen, dan gaat dat niet precies lukken. Dit omdat het werkelijke verband tussen valtijd en hoogte een tweede orde polynoom is, en dus niet met een lineaire functie \n",
    "precies beschreven kan worden. Een iets complexer model zal het hier waarschijnlijk beter doen.\n",
    "\n",
    "De variance is de ingewikkelste oorzaak van een niet-ideale voorspelling op de test set. De achterliggende oorzaak is dat een model getraind wordt op basis van een train set. Maar deze train set is meestal niet volkomen \n",
    "representatief. Vaak is het een steekproef vanuit een grotere populatie. Als een tweede keer een steekproef zou worden getrokken zouden we net andere observaties kunnen krijgen en dus ook andere geleerde parameters in ons model.\n",
    "Soms bestaat de train set zelfs uit de gelabelde data die toevallig beschikbaar zijn. Zelfs in het geval dat de train en test set geconstrueerd zijn uit dezelfde ruwe data set door middel van het random toewijzen aan de \n",
    "train set of de test set, dan nog is er een toevalselement aanwezig. Bij herhaling kan dit leiden tot een andere train set en dus andere geleerde parameters. Indien er veel data beschikbaar is en een model met weinig parameters\n",
    "is de variance meestal geen groot probleem, maar dat wordt anders bij modellen met veel parameters en / of miner data.\n",
    "  \n",
    "De take-away van dit notebook is dat bias en variance zich vaak gedragen als een wip: zodra je de bias omlaag probeert te krijgen door een complexer model te kiezen, gaat de variance omhoog. En zodra je de variance verlaagt \n",
    "met een eenvoudiger model, gaat de bias weer omhoog. Het gaat erom een balans te vinden en te weten dat je niet altijd moet kiezen voor het meest ingewikkelde model. Alleen met meer data kun je meestal beide naar beneden krijgen. Maar voordat we het gaan hebben over bias en variance, eerst wat nuttige kennis over supervised learning algoritmen in het algemeen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd3dc9a-63b3-4507-be6b-c9959c3417d1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "\n",
    "\n",
    "# Opdracht\n",
    "Vraag aan chatGPT, claude of een ander Large Language Model om een uitleg te geven over de bias-variance tradeoff. Vraag ook of het model een stukje code kan schrijven om het te verduidelijken.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b5db6-d2b8-45d0-98a9-ceefb5e6c822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "517760d0-541a-4dd9-973f-966e43c15bbb",
   "metadata": {},
   "source": [
    "## Redeneren over álle supervised learning algoritmen\n",
    "We kijken in dit notebook naar supervised learning algoritmen in het algemeen. Dus niet specifiek naar lineaire regressie, logistische regressie, decision trees, random forest, XGBoost, neurale netwerken, of een ander specifiek algoritme. Nee, we kijken algemener naar wat al deze algoritmes gezamenlijk hebben. Wat willen deze algoritmen bereiken? En welke elementen hebben ze allemaal met elkaar gemeen? Deze manier van kijken gaat ons natuurlijk geen praktische kennis opleveren over een specifiek algoritme. Maar we zullen zien dat door in het algemeen te kijken naar wat deze groep algoritmen doet, we toch een stukje nuttige kennis kunnen afleiden. In dit geval over de bias-variance trade-off. En het leuke van deze kennis is dat het kennis is die voor alle supeervised learning algoritmen geldt. Dus ook als over 20 jaar volkomen nieuwe algoritmen uitgevonden zijn, dan nog blijft deze kennis geldig en waardevol. Het zal je ook een betere data-scientist maken als je het aspect van bias-variance trade-off kent en in je achterhoofd houdt bij het kiezen van een model. Wel is dit onderdeel een moeilijk onderdeel. Want het abstract redeneren is wat lastiger dan het concreet kunnen vertellen over een specifiek algoritmen. Maar laten we van start gaan!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14154d1a-2c09-49eb-97c9-05b3b7e11102",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Het eerste element dat vaak (impliciet) voorkomt: de structuurfunctie\n",
    "De meeste supervised learning algoritmen proberen een relatie te vinden tussen een aantal verklarende variabelen $x_1$, $x_2$, ... , $x_n$ en een targetvariabele $y$. De verklarende variabelen zijn meestal getallen en we houden ervan om ze te schrijven als een vector. Vanaf nu zal ik dus ook de vetgedrukte x gebruiken $\\mathbf{x}$. Als ik dat doe, dan bedoel ik dus de vector met als eerste element $x_1$ en als laatste element $x_n$: $\\mathbf{x}=(x_1,\\ldots,x_n)$. Jullie weten dat we de relatie tussen $\\mathbf{x}$ en $y$ kunnen schrijven als een functie. In dit geval:\n",
    "\n",
    "$$\n",
    "y = f(\\mathbf{x}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Ik noem deze functie in dit notebook de **structuurfunctie** van het supervised learning algoritme. Voor sommige algoritmen zoals lineaire regressie en logistische regressie is de structuurfunctie makkelijk op te schrijven. Voor lineaire regressie geldt bijvoorbeeld:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = c_0 + c_1 \\cdot x_1 + \\ldots + c_n \\cdot x_n \\tag{2}\n",
    "$$\n",
    "\n",
    "Hierbij zijn $c_0, c_1, \\ldots, c_n$ getallen die we *parameters* noemen. Deze moeten geleerd worden uit de data. De parameters kunnen we ook als vector schrijven: $\\mathbf{c}=c_0, \\ldots. c_n$. Maken we nu een nieuwe vector $\\mathbf{x'}$ door aan de vector $\\mathbf{x}$ een extra element met waarde 1 toe te voegen aan het begin, dan kunnen we formule (2) ook korter schrijven als het inproduct van twee vectoren:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x'}) = \\mathbf{c} \\cdot \\mathbf{x'} \\tag{3}\n",
    "$$\n",
    "\n",
    "Meestal is het lastiger om de structuurfunctie op te schrijven. Voor een beslisboom met 5 binaire splits geldt bijvoorbeeld (zie _Hastie, Tibshirani, Friedman: The elements of Statistical Learning (2001) p. 267_):\n",
    "\n",
    "$$\n",
    "g(\\mathbf{x})=\\sum_{m=1}^5 c_m I\\{\\mathbf{x} \\in R_m\\} \\tag{4}\n",
    "$$\n",
    "\n",
    "Het gaat er ons op dit moment niet om om de structuurfunctie te kunnen opschrijven, het gaat er nu om dat voor heel veel supervised learning algoritmen er zo'n structuurfunctie bestaat. Ook al wordt ie vaak niet opgeschreven omdat het nogal ingewikkeld is. Het bestaan van een onderliggende structuurfunctie is een element dat veel supervised learning algoritmen gemeenschappelijk hebben.\n",
    "\n",
    "Eigenlijk zijn de formules (1) tot en met (4) hierboven nog niet compleet. In de praktijk gebeurt het nooit dat de targetvariabele $y$ precies voldoet aan de structuurfunctie. Dus dat $y$ bijvoorbeeld exact voorspelt kan worden met bijvoorbeeld formule (2). Daarom voegen we bij al deze formules nog een extra term toe: $+ \\epsilon$. Deze $\\epsilon$ stelt een (random) getal voor dat zo gekozen wordt zodat de formule precies klopt. Deze extra term wordt ook wel de _errorterm_ genoemd, omdat dit getal de fout is tussen de voorspelling (= uitkomst van de structuurfunctie) en de echte waarde. De juiste schrijfwijze van bijvoorbeeld formule (1) is dus:\n",
    "\n",
    "$$\n",
    "y = f(\\mathbf{x})  + \\epsilon \\tag{5}\n",
    "$$\n",
    "\n",
    "Veel beginnelingen denken dat ze een algoritme moeten kiezen met een gecompliceerde structuurfunctie en veel parameters. Want een simpele structuurfunctie, zoals (3), kan alleen maar simpele relaties modelleren. In dit geval rechte lijnen. Terwijl een ingewikkelder structuurfunctie (bijvoorbeeld met extra termen zoals $+ c_{n+1}x_1^2$ niet alleen rechte lijnen kan modelleren, maar ook kromme lijnen. In dit geval parabolen. Maar dat is niet altijd waar. Dit is de kern van dit notebook. Dus werk snel verder. Maar eerst doen we een opdracht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7930a46-9b0b-48a6-8716-8bf61c86a428",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "\n",
    "\n",
    "# Opdracht\n",
    "Neem de wijn data set. Splits de data set in een train set en test set (30% van de observaties komt in de test set). Voorspel de targetvariabele 'points' op basis van 'price' en 'alcohol' waarbij je lineaire regressie gebruikt.\n",
    "1. Wat is de structuurfunctie nadat je getraind hebt? Geeft de waarden van $c_0$, $c_{price}$ en $c_{alcohol}$.\n",
    "2. Berekenen voor alle wijnen van de test set wat de waarde is van $\\epsilon$. Maak een histogram van de waarden van $\\epsilon$.\n",
    "\n",
    "</div>ht\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716583ca-6e11-4b63-a8fc-dd968a04a38d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "403acbd0-06c1-4e01-ab13-02e862e53938",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Het tweede element dat vaak voorkomt: de loss functie\n",
    "Naast een structuurfunctie, werken de meeste supervised learning algoritmen ook met een loss-functie.\n",
    "Deze is al uitgebreid behandeld. Voor een continue target is de meest voorkomende loss functie de mean squared error loss functie:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i = 1}^n (y_i - f(\\mathbf{x_i}))^2\n",
    "$$\n",
    "\n",
    "Zoals jullie weten kun je de loss functie zowel berekenen over je train set om te kijken of het leren goed gaat, maar ook berekenen op je test set om te kijken of je niet aan het overfitten bent.\n",
    "\n",
    "Voor een nominale target bestaan andere loss functies, zoals de cross entropy loss. \n",
    "\n",
    "We sommen nu even op wat de drie elementen zijn die de meeste supervised learning algoritmen in één of andere vorm hebben:\n",
    "1. structuurfunctie;\n",
    "2. loss functie;\n",
    "3. optimalisatie algoritme.\n",
    "\n",
    "Over het optimalisatie algoritme hebben we het niet gehad, omdat het geen rol speelt voor de Bias-Variance trade-off. Maar voor de volledigheid noemen we twee veel voorkomende optimalisatie algoritmen: Gradient Descent en Newton-Raphson, maar er bestaan er meer. Als je als beginnende data scientist voor de algoritmen die je gebruikt een idee hebt van de structuurfunctie, de gebruikte loss functie en het toegepaste optimalisatie algoritme, dan heb je al een heel goed gevoel waar je mee bezig bent!\n",
    "\n",
    "Voordat we verder gaan met de Bias - Variance trade-off, eerst nog een korte oefening met de loss functie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9bea3c-4122-4119-8ea3-ff707c6245c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "\n",
    "\n",
    "# Opdracht\n",
    "Ga verder met de wijn data set. Berekenen de MSE op je test set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722129be-cc22-4633-ab73-fa4a02cca082",
   "metadata": {},
   "source": [
    "## Bias-Variance tradeoff in een voorbeeld\n",
    "Nu we weer scherp hebben hoe we de loss kunnen uitrekenen op de test set, kunnen we teruggaan naar het eigenlijke onderwerp van dit notebook. De bias-variance tradeoff. Zoals bovenaan in dit notebook is uitgelegd, is de bias een oorzaak van een deel van de loss op de test set en de variance ook. Om hier een beter beeld van te krijgen, gaan we hier nu zelf mee rekenen.\n",
    "\n",
    "In de praktijk kun je niet de bias en de variance in een getal uitdrukken, omdat je niet weet wat het precieze, echte, onderliggende relatie is tussen $y$ en $\\mathbf{x}$. Maar voor dit voorbeeld, bedenken we zelf van een relatie tussen $y$ en $\\mathbf{x}$. Dan kan het wel. Vervolgens gaan we deze relatie schatten met een drietal modellen, van simpel tot wat complexer. Als we meerdere keren een train set genereren en steeds opnieuw een model trainen, dan zullen we zien dat de uitkomst niet steeds hetzelfde is. We zullen zien dat voor eenvoudige modellen het niet zoveel verschil uitmaakt als we opnieuw trainen, maar voor ingewikkelder modellen wel. In ons voorbeeld kunnen we dan zelfs een getal hangen aan bias en variance. Op die manier kun je zelf zien wat er gebeurt als je een complexer model neemt.\n",
    "\n",
    "Om te beginnen willen het begrip _bias_ kwantificeren. In woorden hebben we gezegd dat de bias een afwijking in de voorspelling is, veroorzaakt doordat het gekozen model te eenvoudig is om het werkelijke verband tussen $y$ en $\\mathbf{x}$ te omschrijven. Als we dit willen uitdrukken in een getal, dan kan dat eigenlijk alleen door erg ingewikkelde wiskundige berekeningen uit te voeren met kansverdelingen. We zullen daarom niet de bias uitrekenen, maar een goede benadering van de bias, de _sample bias_. Deze is goed te berekenen als je een aantal keer (bijvoorbeeld 10) achter elkaar hetzelfde model fit, maar steeds op een nieuwe train set. Als we dan het gemiddelde van die voorspellingen nemen en vergelijken met de echte waarde, krijgen we de sample bias. In formulevorm:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8be811-022b-44be-a1c6-0594aa83fb70",
   "metadata": {},
   "source": [
    "$$\n",
    "Sample Bias ^2 = \\frac{1}{n} \\sum_{i=1}^n  (\\widehat{f(\\mathbf{x_i})} - y)^2\n",
    "$$\n",
    "\n",
    "Hier is $\\widehat{f(\\mathbf{x_i})}$ het gemiddelde van die 10 keer trainen op steeds een andere train set. We berekenen het kwadraat omdat er straks een leuk verband blijkt te bestaan tussen de MSE op de test set, de bias in het kwadraat en de variance. Als je niet het kwadraat wilt, dan neem je gewoon de wortel van het gevonden getal.\n",
    "\n",
    "Ook kunnen we de _sample variance_ kwantificeren. In principe is dit de mate waarin de voorspellingen verschillen als je je model traint op een nieuwe train set. In formule vorm:\n",
    "\n",
    "$$\n",
    "Sample Variance =  \\frac{1}{m} \\sum_{j=1}^m \\frac{1}{n} \\sum_{i=1}^n  (f^j(\\mathbf{x_i}) - \\widehat{f(\\mathbf{x_i})})^2\n",
    "$$\n",
    "Hierbij is $m$ het aantal keer dat je je model hertraint en $f^j$ is de voorspelling van het model dat de j-de keer getraind is.\n",
    "\n",
    "Het begint nu allemaal wel moeilijk te worden. Laten we dus snel gaan kijken naar het voorbeeld! We beginnen met het definiëren van een zelf bedachte relatie tussen $\\mathbf{x}$ en $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99526262-f92e-46cd-b489-4091b322cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieer de \"ware\" functie waarmee we de data genereren.\n",
    "# x is een 7-dimensionale vector en we kiezen een combinatie van lineaire, kwadratische en niet-lineaire (sinus, log) termen.\n",
    "def true_function(X):\n",
    "    # X heeft vorm (n_samples, 7)\n",
    "    return (1 \n",
    "            + 2 * X[:, 0]               # lineair effect van feature 1\n",
    "            - 1 * X[:, 1]               # lineair effect van feature 2\n",
    "            + 0.5 * (X[:, 2] ** 2)        # kwadratisch effect van feature 3\n",
    "            + np.sin(X[:, 3])           # niet-lineair effect van feature 4\n",
    "            + np.log(1 + np.abs(X[:, 4]))  # niet-lineair effect van feature 5\n",
    "            \n",
    "            + 0.3 * X[:, 5] * X[:, 6])   # interactieterm tussen feature 6 en 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8053ffc9-bd49-49e8-96f6-8c3395b19f9f",
   "metadata": {},
   "source": [
    "Nu genereren we test data met behulp van deze relatie tussen $y$ en $\\mathbf{x}$. We generen de echte waarden van $y$ ('y_true') en voegen ook een irreducibele fout toe, net als in de praktijk het geval is. Deze target met ruis noemen we 'y_test'. De x-waarden noemen we 'X_test'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba07e40-d3bd-4310-b3db-a5d3b6e1efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stel een vaste seed in voor reproduceerbaarheid\n",
    "np.random.seed(42)\n",
    "\n",
    "# Functie om data te genereren.\n",
    "def generate_data(n_samples=50, noise_std=1.0):\n",
    "    # Genereer 7 features, elk uniform verdeeld tussen -5 en 5.\n",
    "    X = np.random.uniform(-5, 5, size=(n_samples, 7))\n",
    "    # Bereken de \"ware\" y-waarden (zonder ruis)\n",
    "    y_true = true_function(X)\n",
    "    # Voeg normale ruis toe (voor de trainingsdata)\n",
    "    noise = np.random.normal(0, noise_std, size=n_samples)\n",
    "    y = y_true + noise\n",
    "    return X, y, y_true\n",
    "\n",
    "# Genereer een testset (groot genoeg en zonder ruis, zodat we de ware functie kennen)\n",
    "X_test, y_test, y_test_true = generate_data(n_samples=1000, noise_std=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a4b6d-d0c7-4317-a290-83a4637a6842",
   "metadata": {},
   "source": [
    "Nu gaan we een simpel lineaire regressie model trainen om $y$ te voorspellen, enkel op basis van de eerste kolom van de data X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d340e00-44ac-4167-9daa-8b58e68721f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - Simpel model (alleen feature 1):\n",
      "Gemiddelde MSE: 32.31551778490702\n",
      "Gemiddelde bias^2: 30.373946343566\n",
      "Gemiddelde variantie: 1.9415714413410297\n"
     ]
    }
   ],
   "source": [
    "# We herhalen het experiment meerdere keren (bijvoorbeeld 10) met kleine trainingssets\n",
    "n_experiments = 10\n",
    "\n",
    "# Lijsten om de voorspellingen van elk model op te slaan (per experiment)\n",
    "predictions_model1 = []  # Model 1: gebruikt alleen feature 1\n",
    "\n",
    "for i in range(n_experiments):\n",
    "    # Voor de trainingsset kiezen we bewust een klein aantal datapunten (bijvoorbeeld 30)\n",
    "    X_train, y_train, _ = generate_data(n_samples=30, noise_std=1.0)\n",
    "    \n",
    "    # --- Model 1: Simpel model (alleen de eerste feature) ---\n",
    "    model1 = LinearRegression()\n",
    "    model1.fit(X_train[:, 0].reshape(-1, 1), y_train)\n",
    "    pred1 = model1.predict(X_test[:, 0].reshape(-1, 1))\n",
    "    predictions_model1.append(pred1)\n",
    "\n",
    "# Zet de lijst met voorspellingen om in een numpy-array voor verdere berekeningen.\n",
    "# De arrays hebben nu de vorm (n_experiments, n_test_samples)\n",
    "predictions_model1 = np.array(predictions_model1)\n",
    "\n",
    "# Functie om per testpunt de MSE, bias² en variantie te berekenen\n",
    "def compute_bias_variance(predictions, y_true):\n",
    "    # Gemiddelde voorspelling per testpunt over alle experimenten\n",
    "    mean_preds = np.mean(predictions, axis=0)\n",
    "    # Varianties per testpunt\n",
    "    variance = np.var(predictions, axis=0)\n",
    "    # Bias² per testpunt: (gemiddelde voorspelling - ware waarde)²\n",
    "    bias_squared = (mean_preds - y_true) ** 2\n",
    "    # MSE per testpunt: gemiddelde van (voorspelling - ware waarde)²\n",
    "    mse = np.mean((predictions - y_true) ** 2, axis=0)\n",
    "    return mse, bias_squared, variance\n",
    "\n",
    "# Bereken de MSE, bias² en variantie voor het model\n",
    "mse1, bias_sq1, var1 = compute_bias_variance(predictions_model1, y_test_true)\n",
    "\n",
    "\n",
    "# Toon de gemiddelde MSE, bias² en variantie over de gehele testset voor het model\n",
    "print(\"Model 1 - Simpel model (alleen feature 1):\")\n",
    "print(\"Gemiddelde MSE:\", np.mean(mse1))\n",
    "print(\"Gemiddelde bias^2:\", np.mean(bias_sq1))\n",
    "print(\"Gemiddelde variantie:\", np.mean(var1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245fb8fa-af45-49ee-8300-a75f47b6dc3a",
   "metadata": {},
   "source": [
    "Je ziet dat het grootste deel van de MSE van de test set veroorzaakt wordt doordat het model eigenlijk te simpel is. De bias is groot en de variance is klein. Daarom nu de opdracht aan jullie om de volgende twee andere modellen op dezelfde wijze te trainen en te zien wat er dan uitkomt voor de MSE, de bias en de variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cdccde-16a3-4563-ab96-2636f6daafb5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "\n",
    "\n",
    "# Opdracht\n",
    "1. Pas de code hierboven aan voor een lienaire regressie model dat alle features gebruikt.\n",
    "2. Voeg nu aan de code hierboven nog een model toe, namelijk een polynomiale regressie (graad 2) met alle features\n",
    "3. Wat gebeurt er met de MSE. En met de bias en variance? Kun je dit verklaren?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbfae14-f2af-4ff4-b495-ed4faf2ebe5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "259792b8-c7c5-4e8b-ae00-c005d4c9cf32",
   "metadata": {},
   "source": [
    "## Uitwerking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aacfe93d-0251-451d-ae90-ee7f202e0827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - Simpel model (alleen feature 1):\n",
      "Gemiddelde MSE: 32.31551778490702\n",
      "Gemiddelde bias^2: 30.373946343566\n",
      "Gemiddelde variantie: 1.9415714413410297\n",
      "\n",
      "Model 2 - Lineaire regressie (alle features):\n",
      "Gemiddelde MSE: 29.26822589385843\n",
      "Gemiddelde bias^2: 22.231620155439145\n",
      "Gemiddelde variantie: 7.036605738419279\n",
      "\n",
      "Model 3 - Polynomiale regressie (graad 2):\n",
      "Gemiddelde MSE: 73.37615727893854\n",
      "Gemiddelde bias^2: 16.743715120660294\n",
      "Gemiddelde variantie: 56.632442158278245\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# We herhalen het experiment meerdere keren (bijvoorbeeld 10) met kleine trainingssets\n",
    "n_experiments = 10\n",
    "\n",
    "# Lijsten om de voorspellingen van elk model op te slaan (per experiment)\n",
    "predictions_model1 = []  # Model 1: gebruikt alleen feature 1\n",
    "predictions_model2 = []  # Model 2: lineaire regressie met alle features\n",
    "predictions_model3 = []  # Model 3: polynomiale regressie (graad 2) met alle features\n",
    "\n",
    "for i in range(n_experiments):\n",
    "    # Voor de trainingsset kiezen we bewust een klein aantal datapunten (bijvoorbeeld 30)\n",
    "    X_train, y_train, _ = generate_data(n_samples=30, noise_std=1.0)\n",
    "    \n",
    "    # --- Model 1: Simpel model (alleen de eerste feature) ---\n",
    "    model1 = LinearRegression()\n",
    "    model1.fit(X_train[:, 0].reshape(-1, 1), y_train)\n",
    "    pred1 = model1.predict(X_test[:, 0].reshape(-1, 1))\n",
    "    predictions_model1.append(pred1)\n",
    "    \n",
    "    # --- Model 2: Lineaire regressie met alle 7 features ---\n",
    "    model2 = LinearRegression()\n",
    "    model2.fit(X_train, y_train)\n",
    "    pred2 = model2.predict(X_test)\n",
    "    predictions_model2.append(pred2)\n",
    "    \n",
    "    # --- Model 3: Polynomiale regressie (graad 2) met alle features ---\n",
    "    model3 = make_pipeline(PolynomialFeatures(degree=2, include_bias=False), LinearRegression())\n",
    "    model3.fit(X_train, y_train)\n",
    "    pred3 = model3.predict(X_test)\n",
    "    predictions_model3.append(pred3)\n",
    "\n",
    "# Zet de lijst met voorspellingen om in een numpy-array voor verdere berekeningen.\n",
    "# De arrays hebben nu de vorm (n_experiments, n_test_samples)\n",
    "predictions_model1 = np.array(predictions_model1)\n",
    "predictions_model2 = np.array(predictions_model2)\n",
    "predictions_model3 = np.array(predictions_model3)\n",
    "\n",
    "# Functie om per testpunt de MSE, bias² en variantie te berekenen\n",
    "def compute_bias_variance(predictions, y_true):\n",
    "    # Gemiddelde voorspelling per testpunt over alle experimenten\n",
    "    mean_preds = np.mean(predictions, axis=0)\n",
    "    # Varianties per testpunt\n",
    "    variance = np.var(predictions, axis=0)\n",
    "    # Bias² per testpunt: (gemiddelde voorspelling - ware waarde)²\n",
    "    bias_squared = (mean_preds - y_true) ** 2\n",
    "    # MSE per testpunt: gemiddelde van (voorspelling - ware waarde)²\n",
    "    mse = np.mean((predictions - y_true) ** 2, axis=0)\n",
    "    return mse, bias_squared, variance\n",
    "\n",
    "# Bereken de MSE, bias² en variantie voor elk model\n",
    "mse1, bias_sq1, var1 = compute_bias_variance(predictions_model1, y_test_true)\n",
    "mse2, bias_sq2, var2 = compute_bias_variance(predictions_model2, y_test_true)\n",
    "mse3, bias_sq3, var3 = compute_bias_variance(predictions_model3, y_test_true)\n",
    "\n",
    "# Toon de gemiddelde MSE, bias² en variantie over de gehele testset per model\n",
    "print(\"Model 1 - Simpel model (alleen feature 1):\")\n",
    "print(\"Gemiddelde MSE:\", np.mean(mse1))\n",
    "print(\"Gemiddelde bias^2:\", np.mean(bias_sq1))\n",
    "print(\"Gemiddelde variantie:\", np.mean(var1))\n",
    "print(\"\\nModel 2 - Lineaire regressie (alle features):\")\n",
    "print(\"Gemiddelde MSE:\", np.mean(mse2))\n",
    "print(\"Gemiddelde bias^2:\", np.mean(bias_sq2))\n",
    "print(\"Gemiddelde variantie:\", np.mean(var2))\n",
    "print(\"\\nModel 3 - Polynomiale regressie (graad 2):\")\n",
    "print(\"Gemiddelde MSE:\", np.mean(mse3))\n",
    "print(\"Gemiddelde bias^2:\", np.mean(bias_sq3))\n",
    "print(\"Gemiddelde variantie:\", np.mean(var3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cfeea3-fbd2-4683-a598-bee4c30f1744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
