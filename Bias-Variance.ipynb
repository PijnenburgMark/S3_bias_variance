{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433c4533-41d0-4333-a022-a41d4ac092b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Bias - Variance Tradeoff "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28296a-31bf-4e37-8ab5-5981ab7d60e2",
   "metadata": {},
   "source": [
    "We beginnen met de initialisatie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96a11b2-9a06-4b5a-a7e5-0228138705e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install numpy scikit-learn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a194be-715d-4b88-aaf0-6691a2896bee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In dit workbook gaan we het hebben over de 3 verschillende oorzaken waarom een voorspellend model niet perfect voorspelt op een test set. De 3 redenen zijn:\n",
    "1. de irreducibele fout,\n",
    "2. de bias,\n",
    "3. de variance.\n",
    "\n",
    "De irreducibele fout is lastig te voorkomen. Als we een targetvariabele $y$ voorspellen op basis van $x$, dan kan het zijn dat er nog veel andere factoren zijn die $y$ ook beïnvloeden, maar waar we geen data van hebben. \n",
    "Als we bijvoorbeeld het inkomen voorspellen op basis van het opleidingsniveau, krijgen we nooit een perfecte voorspelling, omdat bijvoorbeeld leeftijd ook een rol speelt. \n",
    "Dit wordt meestal al duidelijk in de data set: er komen twee personen voor met hetzelfde opleidingsniveau en toch hebben beiden een ander inkomen. Dit heet de irreducibele fout. De enige manier om de irreducibele fout \n",
    "kleiner te maken is om meer voorspellende variabelen mee te nemen, maar dan moet je daar dus ook data van hebben. \n",
    "\n",
    "De bias is een afwijking in de voorspelling, omdat het gekozen model te eenvoudig is om het werkelijke verband tussen $x$ en $y$ precies te beschrijven. Als je bijvoorbeeld met lineaire regressie de tijd wilt voorspellen \n",
    "dat een voorwerp nodig heeft om van een hooge $h$ op de vloer te vallen, dan gaat dat niet precies lukken. Dit omdat het werkelijke verband tussen valtijd en hoogte een tweede orde polynoom is, en dus niet met een lineaire functie \n",
    "precies beschreven kan worden. Een iets complexer model zal het hier waarschijnlijk beter doen.\n",
    "\n",
    "De variance is de ingewikkelste oorzaak van een niet-ideale voorspelling op de test set. De achterliggende oorzaak is dat een model getraind wordt op basis van een train set. Maar deze train set is meestal niet volkomen \n",
    "representatief. Vaak is het een steekproef vanuit een grotere populatie. Als een tweede keer een steekproef zou worden getrokken zouden we net andere observaties kunnen krijgen en dus ook andere geleerde parameters in ons model.\n",
    "Soms bestaat de train set zelfs uit de gelabelde data die toevallig beschikbaar zijn. Zelfs in het geval dat de train en test set geconstrueerd zijn uit dezelfde ruwe data set door middel van het random toewijzen aan de \n",
    "train set of de test set, dan nog is er een toevalselement aanwezig. Bij herhaling kan dit leiden tot een andere train set en dus andere geleerde parameters. Indien er veel data beschikbaar is en een model met weinig parameters\n",
    "is de variance meestal geen groot probleem, maar dat wordt anders bij modellen met veel parameters en / of miner data.\n",
    "  \n",
    "De take-away van dit notebook is dat bias en variance zich vaak gedragen als een wip: zodra je de bias omlaag probeert te krijgen door een complexer model te kiezen, gaat de variance omhoog. En zodra je de variance verlaagt \n",
    "met een eenvoudiger model, gaat de bias weer omhoog. Het gaat erom een balans te vinden en te weten dat je niet altijd moet kiezen voor het meest ingewikkelde model. Alleen met meer data kun je meestal beide naar beneden krijgen. Maar voordat we het gaan hebben over bias en variance, eerst wat nuttige kennis over supervised learning algoritmen in het algemeen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd3dc9a-63b3-4507-be6b-c9959c3417d1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "\n",
    "\n",
    "# Opdracht\n",
    "Vraag aan chatGPT, claude of een ander Large Language Model om een uitleg te geven over de bias-variance tradeoff. Vraag ook of het model een stukje code kan schrijven om het te verduidelijken.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b5db6-d2b8-45d0-98a9-ceefb5e6c822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "517760d0-541a-4dd9-973f-966e43c15bbb",
   "metadata": {},
   "source": [
    "## Redeneren over álle supervised learning algoritmen\n",
    "We kijken in dit notebook naar supervised learning algoritmen in het algemeen. Dus niet specifiek naar lineaire regressie, logistische regressie, decision trees, random forest, XGBoost, neurale netwerken, of een ander specifiek algoritme. Nee, we kijken algemener naar wat al deze algoritmes gezamenlijk hebben. Wat willen deze algoritmen bereiken? En welke elementen hebben ze allemaal met elkaar gemeen? Deze manier van kijken gaat ons natuurlijk geen praktische kennis opleveren over een specifiek algoritme. Maar we zullen zien dat door in het algemeen te kijken naar wat deze groep algoritmen doet, we toch een stukje nuttige kennis kunnen afleiden. In dit geval over de bias-variance trade-off. En het leuke van deze kennis is dat het kennis is die voor alle supeervised learning algoritmen geldt. Dus ook als over 20 jaar volkomen nieuwe algoritmen uitgevonden zijn, dan nog blijft deze kennis geldig en waardevol. Het zal je ook een betere data-scientist maken als je het aspect van bias-variance trade-off kent en in je achterhoofd houdt bij het kiezen van een model. Wel is dit onderdeel een moeilijk onderdeel. Want het abstract redeneren is wat lastiger dan het concreet kunnen vertellen over een specifiek algoritmen. Maar laten we van start gaan!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14154d1a-2c09-49eb-97c9-05b3b7e11102",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Het eerste element dat vaak (impliciet) voorkomt: de structuurfunctie\n",
    "De meeste supervised learning algoritmen proberen een relatie te vinden tussen een aantal verklarende variabelen $x_1$, $x_2$, ... , $x_n$ en een targetvariabele $y$. De verklarende variabelen zijn meestal getallen en we houden ervan om ze te schrijven als een vector. Vanaf nu zal ik dus ook de vetgedrukte x gebruiken $\\mathbf{x}$. Als ik dat doe, dan bedoel ik dus de vector met als eerste element $x_1$ en als laatste element $x_n$: $\\mathbf{x}=(x_1,\\ldots,x_n)$. Jullie weten dat we de relatie tussen $\\mathbf{x}$ en $y$ kunnen schrijven als een functie. In dit geval:\n",
    "\n",
    "$$\n",
    "y = f(\\mathbf{x}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Ik noem deze functie in dit notebook de **structuurfunctie** van het supervised learning algoritme. Voor sommige algoritmen zoals lineaire regressie en logistische regressie is de structuurfunctie makkelijk op te schrijven. Voor lineaire regressie geldt bijvoorbeeld:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = c_0 + c_1 \\cdot x_1 + \\ldots + c_n \\cdot x_n \\tag{2}\n",
    "$$\n",
    "\n",
    "Hierbij zijn $c_0, c_1, \\ldots, c_n$ getallen die we *parameters* noemen. Deze moeten geleerd worden uit de data. De parameters kunnen we ook als vector schrijven: $\\mathbf{c}=c_0, \\ldots. c_n$. Maken we nu een nieuwe vector $\\mathbf{x'}$ door aan de vector $\\mathbf{x}$ een extra element met waarde 1 toe te voegen aan het begin, dan kunnen we formule (2) ook korter schrijven als het inproduct van twee vectoren:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x'}) = \\mathbf{c} \\cdot \\mathbf{x'} \\tag{3}\n",
    "$$\n",
    "\n",
    "Meestal is het lastiger om de structuurfunctie op te schrijven. Voor een beslisboom met 5 binaire splits geldt bijvoorbeeld (zie _Hastie, Tibshirani, Friedman: The elements of Statistical Learning (2001) p. 267_):\n",
    "\n",
    "$$\n",
    "g(\\mathbf{x})=\\sum_{m=1}^5 c_m I\\{\\mathbf{x} \\in R_m\\} \\tag{4}\n",
    "$$\n",
    "\n",
    "Het gaat er ons op dit moment niet om om de structuurfunctie te kunnen opschrijven, het gaat er nu om dat voor heel veel supervised learning algoritmen er zo'n structuurfunctie bestaat. Ook al wordt ie vaak niet opgeschreven omdat het nogal ingewikkeld is. Het bestaan van een onderliggende structuurfunctie is een element dat veel supervised learning algoritmen gemeenschappelijk hebben.\n",
    "\n",
    "Eigenlijk zijn de formules (1) tot en met (4) hierboven nog niet compleet. In de praktijk gebeurt het nooit dat de targetvariabele $y$ precies voldoet aan de structuurfunctie. Dus dat $y$ bijvoorbeeld exact voorspelt kan worden met bijvoorbeeld formule (2). Daarom voegen we bij al deze formules nog een extra term toe: $+ \\epsilon$. Deze $\\epsilon$ stelt een (random) getal voor dat zo gekozen wordt zodat de formule precies klopt. Deze extra term wordt ook wel de _errorterm_ genoemd, omdat dit getal de fout is tussen de voorspelling (= uitkomst van de structuurfunctie) en de echte waarde. De juiste schrijfwijze van bijvoorbeeld formule (1) is dus:\n",
    "\n",
    "$$\n",
    "y = f(\\mathbf{x})  + \\epsilon \\tag{5}\n",
    "$$\n",
    "\n",
    "Veel beginnelingen denken dat ze een algoritme moeten kiezen met een gecompliceerde structuurfunctie en veel parameters. Want een simpele structuurfunctie, zoals (3), kan alleen maar simpele relaties modelleren. In dit geval rechte lijnen. Terwijl een ingewikkelder structuurfunctie (bijvoorbeeld met extra termen zoals $+ c_{n+1}x_1^2$ niet alleen rechte lijnen kan modelleren, maar ook kromme lijnen. In dit geval parabolen. Maar dat is niet altijd waar. Dit is de kern van dit notebook. Dus werk snel verder. Maar eerst doen we een opdracht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7930a46-9b0b-48a6-8716-8bf61c86a428",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "\n",
    "\n",
    "# Opdracht\n",
    "Neem de wijn data set. Splits de data set in een train set en test set (30% van de observaties komt in de test set). Voorspel de targetvariabele 'points' op basis van 'price' en 'alcohol' waarbij je lineaire regressie gebruikt.\n",
    "1. Wat is de structuurfunctie nadat je getraind hebt? Geeft de waarden van $c_0$, $c_{price}$ en $c_{alcohol}$.\n",
    "2. Berekenen voor alle wijnen van de test set wat de waarde is van $\\epsilon$. Maak een histogram van de waarden van $\\epsilon$.\n",
    "\n",
    "</div>ht\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716583ca-6e11-4b63-a8fc-dd968a04a38d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "403acbd0-06c1-4e01-ab13-02e862e53938",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Het tweede element dat vaak voorkomt: de loss functie\n",
    "Naast een structuurfunctie, werken de meeste supervised learning algoritmen ook met een loss-functie.\n",
    "Deze is al uitgebreid behandeld. Voor een continue target is de meest voorkomende loss functie de mean squared error loss functie:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i = 1}^n (y_i - f(\\mathbf{x_i}))^2\n",
    "$$\n",
    "\n",
    "Zoals jullie weten kun je de loss functie zowel berekenen over je train set om te kijken of het leren goed gaat, maar ook berekenen op je test set om te kijken of je niet aan het overfitten bent.\n",
    "\n",
    "Voor een nominale target bestaan andere loss functies, zoals de cross entropy loss. \n",
    "\n",
    "We sommen nu even op wat de drie elementen zijn die de meeste supervised learning algoritmen in één of andere vorm hebben:\n",
    "1. structuurfunctie;\n",
    "2. loss functie;\n",
    "3. optimalisatie algoritme.\n",
    "\n",
    "Over het optimalisatie algoritme hebben we het niet gehad, omdat het geen rol speelt voor de Bias-Variance trade-off. Maar voor de volledigheid noemen we twee veel voorkomende optimalisatie algoritmen: Gradient Descent en Newton-Raphson, maar er bestaan er meer. Als je als beginnende data scientist voor de algoritmen die je gebruikt een idee hebt van de structuurfunctie, de gebruikte loss functie en het toegepaste optimalisatie algoritme, dan heb je al een heel goed gevoel waar je mee bezig bent!\n",
    "\n",
    "Voordat we verder gaan met de Bias - Variance trade-off, eerst nog een korte oefening met de loss functie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9bea3c-4122-4119-8ea3-ff707c6245c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "\n",
    "\n",
    "# Opdracht\n",
    "Ga verder met de wijn data set. Berekenen de MSE op je test set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722129be-cc22-4633-ab73-fa4a02cca082",
   "metadata": {},
   "source": [
    "## Bias-Variance tradeoff in een voorbeeld\n",
    "Nu we weer scherp hebben hoe we de loss kunnen uitrekenen op de test set, kunnen we teruggaan naar het eigenlijke onderwerp van dit notebook. De bias-variance tradeoff. Zoals bovenaan in dit notebook is uitgelegd, is de bias een oorzaak van een deel van de loss op de test set en de variance ook. Om hier een beter beeld van te krijgen, verzinnen we nu zelf een verband tussen $y$ en $\\mathbf{x}$. Vervolgens gaan we dit verband schatten met een drietal modellen, van heel simpel tot wat complexer. We zullen voor elk van die modellen uitrekenen wat de mse op de test set is en ook de bias en variance uitrekenen op de test set.\n",
    "\n",
    "Om te beginnen willen het begrip bias kwantificeren. In woorden hebben we gezegd dat de bias een afwijking in de voorspelling is, veroorzaakt doordat het gekozen model te eenvoudig is om het werkelijke verband tussen $y$ en $\\mathbf{x}$ te omschrijven. Als we dit willen uitdrukken in een getal, dan kunnen we een aantal keer (bijvoorbeeld 10) hetzelfde model fitten, maar steeds op een nieuwe train set. Als we dan het gemiddelde van die voorspellingen nemen en vergelijken met de echte waarde, krijgen we een beeld van wat maximaal mogelijk is met ons model. In formulevorm is de bias daarom uit te drukken als:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8be811-022b-44be-a1c6-0594aa83fb70",
   "metadata": {},
   "source": [
    "$$\n",
    "Bias ^2 = \\frac{1}{n} \\sum_{i=1}^n  (\\widehat{f(\\mathbf{x_i})} - y)^2\n",
    "$$\n",
    "\n",
    "Hier is $\\widehat{f(\\mathbf{x_i})}$ het gemiddelde van die 10 keer trainen op steeds een andere train set. We berekenen het kwadraat omdat er straks blijkt dat er een mooie formule bestaat tussen de MSE op de test set en de bias in het kwadraat en de variantie.\n",
    "\n",
    "Ook kunnen we de variance kwantificeren:\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "In de praktijk kun je niet de bias en de variance in een getal uitdrukken, omdat je niet weet wat het precieze, echte, onderliggende verband is tussen $y$ en $\\mathbf{x}$. Maar hier wel.\n",
    "We starten met het zelf bedenken van een verband tussen $y$ en $\\mathbf{x}$. Als je het leuk vindt, kun je deze code aanpassen en zelf experimenteren met wat er gebeurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99526262-f92e-46cd-b489-4091b322cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieer de \"ware\" functie waarmee we de data genereren.\n",
    "# x is een 7-dimensionale vector en we kiezen een combinatie van lineaire, kwadratische en niet-lineaire (sinus, log) termen.\n",
    "def true_function(X):\n",
    "    # X heeft vorm (n_samples, 7)\n",
    "    return (1 \n",
    "            + 2 * X[:, 0]               # lineair effect van feature 1\n",
    "            - 1 * X[:, 1]               # lineair effect van feature 2\n",
    "            + 0.5 * (X[:, 2] ** 2)        # kwadratisch effect van feature 3\n",
    "            + np.sin(X[:, 3])           # niet-lineair effect van feature 4\n",
    "            + np.log(1 + np.abs(X[:, 4]))  # niet-lineair effect van feature 5\n",
    "            \n",
    "            + 0.3 * X[:, 5] * X[:, 6])   # interactieterm tussen feature 6 en 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8053ffc9-bd49-49e8-96f6-8c3395b19f9f",
   "metadata": {},
   "source": [
    "Nu genereren we test data met behulp van deze relatie tussen $y$ en $\\mathbf{x}$. We generen de echte waarden van $y$ ('y_true') en voegen ook een irreducibele fout toe, net als in de praktijk het geval is. Deze target met ruis noemen we 'y_test'. De x-waarden noemen we 'X_test'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba07e40-d3bd-4310-b3db-a5d3b6e1efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stel een vaste seed in voor reproduceerbaarheid\n",
    "np.random.seed(42)\n",
    "\n",
    "# Functie om data te genereren.\n",
    "def generate_data(n_samples=50, noise_std=1.0):\n",
    "    # Genereer 7 features, elk uniform verdeeld tussen -5 en 5.\n",
    "    X = np.random.uniform(-5, 5, size=(n_samples, 7))\n",
    "    # Bereken de \"ware\" y-waarden (zonder ruis)\n",
    "    y_true = true_function(X)\n",
    "    # Voeg normale ruis toe (voor de trainingsdata)\n",
    "    noise = np.random.normal(0, noise_std, size=n_samples)\n",
    "    y = y_true + noise\n",
    "    return X, y, y_true\n",
    "\n",
    "# Genereer een testset (groot genoeg en zonder ruis, zodat we de ware functie kennen)\n",
    "X_test, y_test, y_test_true = generate_data(n_samples=1000, noise_std=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cdccde-16a3-4563-ab96-2636f6daafb5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "\n",
    "\n",
    "# Opdracht\n",
    "1. Genereer nu zelf een train set met 30 observaties. Gebruik de functie generate_data en neem noise_std=1.0\n",
    "2. Fit nu een lineaire regressie model op deze train set, waarbij je alleen maar de eerste kolom van X gebruikt. Dit is ons meest simpele model.\n",
    "3. Gebruik je model om de test set te scoren.\n",
    "4. Bereken nu de MSE van de test set.\n",
    "5. Gebruik y_true \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d3539-bf6b-477f-a92c-e9ac9ca5482e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aacfe93d-0251-451d-ae90-ee7f202e0827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - Simpel model (alleen feature 1):\n",
      "Gemiddelde MSE: 32.31551778490702\n",
      "Gemiddelde bias^2: 30.373946343566\n",
      "Gemiddelde variantie: 1.9415714413410297\n",
      "\n",
      "Model 2 - Lineaire regressie (alle features):\n",
      "Gemiddelde MSE: 29.26822589385843\n",
      "Gemiddelde bias^2: 22.231620155439145\n",
      "Gemiddelde variantie: 7.036605738419279\n",
      "\n",
      "Model 3 - Polynomiale regressie (graad 2):\n",
      "Gemiddelde MSE: 73.37615727893854\n",
      "Gemiddelde bias^2: 16.743715120660294\n",
      "Gemiddelde variantie: 56.632442158278245\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# We herhalen het experiment meerdere keren (bijvoorbeeld 10) met kleine trainingssets\n",
    "n_experiments = 10\n",
    "\n",
    "# Lijsten om de voorspellingen van elk model op te slaan (per experiment)\n",
    "predictions_model1 = []  # Model 1: gebruikt alleen feature 1\n",
    "predictions_model2 = []  # Model 2: lineaire regressie met alle features\n",
    "predictions_model3 = []  # Model 3: polynomiale regressie (graad 2) met alle features\n",
    "\n",
    "for i in range(n_experiments):\n",
    "    # Voor de trainingsset kiezen we bewust een klein aantal datapunten (bijvoorbeeld 30)\n",
    "    X_train, y_train, _ = generate_data(n_samples=30, noise_std=1.0)\n",
    "    \n",
    "    # --- Model 1: Simpel model (alleen de eerste feature) ---\n",
    "    model1 = LinearRegression()\n",
    "    model1.fit(X_train[:, 0].reshape(-1, 1), y_train)\n",
    "    pred1 = model1.predict(X_test[:, 0].reshape(-1, 1))\n",
    "    predictions_model1.append(pred1)\n",
    "    \n",
    "    # --- Model 2: Lineaire regressie met alle 7 features ---\n",
    "    model2 = LinearRegression()\n",
    "    model2.fit(X_train, y_train)\n",
    "    pred2 = model2.predict(X_test)\n",
    "    predictions_model2.append(pred2)\n",
    "    \n",
    "    # --- Model 3: Polynomiale regressie (graad 2) met alle features ---\n",
    "    model3 = make_pipeline(PolynomialFeatures(degree=2, include_bias=False), LinearRegression())\n",
    "    model3.fit(X_train, y_train)\n",
    "    pred3 = model3.predict(X_test)\n",
    "    predictions_model3.append(pred3)\n",
    "\n",
    "# Zet de lijst met voorspellingen om in een numpy-array voor verdere berekeningen.\n",
    "# De arrays hebben nu de vorm (n_experiments, n_test_samples)\n",
    "predictions_model1 = np.array(predictions_model1)\n",
    "predictions_model2 = np.array(predictions_model2)\n",
    "predictions_model3 = np.array(predictions_model3)\n",
    "\n",
    "# Functie om per testpunt de MSE, bias² en variantie te berekenen\n",
    "def compute_bias_variance(predictions, y_true):\n",
    "    # Gemiddelde voorspelling per testpunt over alle experimenten\n",
    "    mean_preds = np.mean(predictions, axis=0)\n",
    "    # Varianties per testpunt\n",
    "    variance = np.var(predictions, axis=0)\n",
    "    # Bias² per testpunt: (gemiddelde voorspelling - ware waarde)²\n",
    "    bias_squared = (mean_preds - y_true) ** 2\n",
    "    # MSE per testpunt: gemiddelde van (voorspelling - ware waarde)²\n",
    "    mse = np.mean((predictions - y_true) ** 2, axis=0)\n",
    "    return mse, bias_squared, variance\n",
    "\n",
    "# Bereken de MSE, bias² en variantie voor elk model\n",
    "mse1, bias_sq1, var1 = compute_bias_variance(predictions_model1, y_test_true)\n",
    "mse2, bias_sq2, var2 = compute_bias_variance(predictions_model2, y_test_true)\n",
    "mse3, bias_sq3, var3 = compute_bias_variance(predictions_model3, y_test_true)\n",
    "\n",
    "# Toon de gemiddelde MSE, bias² en variantie over de gehele testset per model\n",
    "print(\"Model 1 - Simpel model (alleen feature 1):\")\n",
    "print(\"Gemiddelde MSE:\", np.mean(mse1))\n",
    "print(\"Gemiddelde bias^2:\", np.mean(bias_sq1))\n",
    "print(\"Gemiddelde variantie:\", np.mean(var1))\n",
    "print(\"\\nModel 2 - Lineaire regressie (alle features):\")\n",
    "print(\"Gemiddelde MSE:\", np.mean(mse2))\n",
    "print(\"Gemiddelde bias^2:\", np.mean(bias_sq2))\n",
    "print(\"Gemiddelde variantie:\", np.mean(var2))\n",
    "print(\"\\nModel 3 - Polynomiale regressie (graad 2):\")\n",
    "print(\"Gemiddelde MSE:\", np.mean(mse3))\n",
    "print(\"Gemiddelde bias^2:\", np.mean(bias_sq3))\n",
    "print(\"Gemiddelde variantie:\", np.mean(var3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cfeea3-fbd2-4683-a598-bee4c30f1744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
